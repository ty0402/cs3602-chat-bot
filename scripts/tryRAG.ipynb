{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu transformers torch numpy tqdm sentence-transformers faiss-gpu \n",
    "!pip install torch --extra-index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# 设置日志\n",
    "logging.basicConfig(filename='chatbot.log', level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s:%(message)s')\n",
    "\n",
    "def chat(query, tokenizer, model, device, index, cleaned_documents, top_k=5):\n",
    "    retrieved_docs = retrieve_documents(query, tokenizer, model, device, index, cleaned_documents, top_k)\n",
    "    answer = generate_answer(query, retrieved_docs, tokenizer, model, device)\n",
    "    # 记录日志\n",
    "    logging.info(f\"Query: {query}\")\n",
    "    logging.info(f\"Retrieved Docs: {retrieved_docs}\")\n",
    "    logging.info(f\"Answer: {answer}\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# 1. 加载和预处理文档\n",
    "def load_documents(directory_path, chunk_size=500, overlap=50):\n",
    "    \"\"\"\n",
    "    加载文档并将其拆分为多个块。\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): 文档目录路径。\n",
    "        chunk_size (int): 每个块的字符数。\n",
    "        overlap (int): 相邻块之间的重叠字符数。\n",
    "\n",
    "    Returns:\n",
    "        list: 拆分后的文档块列表。\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                content = content.replace('\\n', ' ').strip()\n",
    "                if content:\n",
    "                    # 使用滑动窗口拆分\n",
    "                    for i in range(0, len(content), chunk_size - overlap):\n",
    "                        chunk = content[i:i + chunk_size]\n",
    "                        chunk = chunk.strip()\n",
    "                        if chunk:\n",
    "                            documents.append(chunk)\n",
    "    return documents\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    清洗文本，去除多余的空格和特殊字符。\n",
    "\n",
    "    Args:\n",
    "        text (str): 原始文本。\n",
    "\n",
    "    Returns:\n",
    "        str: 清洗后的文本。\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\u4e00-\\u9fa5A-Za-z0-9\\s\\.,，。!?]', '', text)\n",
    "    return text\n",
    "\n",
    "# 2. 生成嵌入向量\n",
    "def get_embedding(text, embedding_model):\n",
    "    \"\"\"\n",
    "    生成文本的嵌入向量。\n",
    "\n",
    "    Args:\n",
    "        text (str): 输入文本。\n",
    "        embedding_model (SentenceTransformer): 句子嵌入模型。\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 归一化后的嵌入向量。\n",
    "    \"\"\"\n",
    "    embedding = embedding_model.encode(text, normalize_embeddings=True)\n",
    "    return embedding\n",
    "\n",
    "# 3. 检索相关文档块\n",
    "def retrieve_documents(query, embedding_model, index, cleaned_documents, top_k=2, threshold=0.3):\n",
    "    \"\"\"\n",
    "    根据查询检索最相关的文档块，并设置阈值过滤不相关的文档。\n",
    "\n",
    "    Args:\n",
    "        query (str): 用户查询。\n",
    "        embedding_model (SentenceTransformer): 句子嵌入模型。\n",
    "        index (faiss.Index): FAISS索引。\n",
    "        cleaned_documents (list): 清洗后的文档块列表。\n",
    "        top_k (int): 检索的相关文档块数量。\n",
    "        threshold (float): 文档相关性的最低阈值。\n",
    "\n",
    "    Returns:\n",
    "        list: 检索到的相关文档块列表。\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query, embedding_model)\n",
    "    query_embedding = np.expand_dims(query_embedding, axis=0).astype('float32')\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    retrieved_docs = []\n",
    "    for i, dist in zip(indices[0], distances[0]):\n",
    "        if dist >= threshold:\n",
    "            retrieved_docs.append(cleaned_documents[i])\n",
    "    \n",
    "    # 保证最多返回 3 个文档块\n",
    "    retrieved_docs = retrieved_docs[:3]\n",
    "    \n",
    "    # 打印检索结果\n",
    "    print(\"检索到的文档块：\")\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        print(f\"文档块 {i+1}: {doc[:200]}...\")  # 仅显示前200字符\n",
    "\n",
    "    return retrieved_docs\n",
    "\n",
    "# 4. 生成回答\n",
    "class StopOnToken(StoppingCriteria):\n",
    "    def __init__(self, stop_token_id):\n",
    "        self.stop_token_id = stop_token_id\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        if input_ids[0][-1] == self.stop_token_id:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "def generate_answer(query, retrieved_docs, tokenizer, model, device, max_new_tokens=256):\n",
    "    \"\"\"\n",
    "    根据查询和检索到的文档块生成回答。\n",
    "\n",
    "    Args:\n",
    "        query (str): 用户查询。\n",
    "        retrieved_docs (list): 检索到的相关文档块。\n",
    "        tokenizer (AutoTokenizer): 生成模型的分词器。\n",
    "        model (AutoModelForCausalLM): 生成模型。\n",
    "        device (torch.device): 设备（CPU或GPU）。\n",
    "        max_new_tokens (int): 生成回答的最大新令牌数。\n",
    "\n",
    "    Returns:\n",
    "        str: 生成的回答。\n",
    "    \"\"\"\n",
    "    # 限制每个文档块的长度，避免上下文过长\n",
    "    truncated_docs = []\n",
    "    for doc in retrieved_docs:\n",
    "        truncated_doc = doc[:500]  # 取每个文档块的前500个字符\n",
    "        truncated_docs.append(truncated_doc)\n",
    "\n",
    "    context = \"\\n---\\n\".join(truncated_docs)  # 使用分隔符区分不同文档块\n",
    "    input_text = f\"问题：{query}\\n相关文档：\\n{context}\\n回答：\"\n",
    "\n",
    "    # 编码输入\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', truncation=True, max_length=1024)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # 设置 pad_token_id，如果模型有 pad_token_id，则使用它；否则使用 eos_token_id\n",
    "    pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "\n",
    "    # 定义自定义停止条件\n",
    "    stopping_criteria = StoppingCriteriaList([StopOnToken(tokenizer.eos_token_id)])\n",
    "\n",
    "    # 生成回答，仅设置 max_new_tokens，移除 max_length\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,  # 如果希望增加多样性，可以设置为 True\n",
    "            num_beams=3,  # 设置 beam 数量\n",
    "            pad_token_id=pad_token_id,  # 明确设置 pad_token_id\n",
    "            eos_token_id=tokenizer.eos_token_id,  # 设置 eos_token_id\n",
    "            no_repeat_ngram_size=2,  # 防止重复\n",
    "            early_stopping=True,  # 启用早停\n",
    "            stopping_criteria=stopping_criteria  # 使用自定义停止条件\n",
    "        )\n",
    "\n",
    "    # 解码生成的文本\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # 提取回答部分（假设回答以\"回答：\"开头）\n",
    "    if \"回答：\" in answer:\n",
    "        answer = answer.split(\"回答：\")[-1].strip()\n",
    "    return answer\n",
    "\n",
    "# 5. 整合聊天流程\n",
    "def chat(query, embedding_model, tokenizer, model, device, index, cleaned_documents, top_k=2, threshold=0.3):\n",
    "    \"\"\"\n",
    "    处理用户查询并生成回答。\n",
    "\n",
    "    Args:\n",
    "        query (str): 用户查询。\n",
    "        embedding_model (SentenceTransformer): 句子嵌入模型。\n",
    "        tokenizer (AutoTokenizer): 生成模型的分词器。\n",
    "        model (AutoModelForCausalLM): 生成模型。\n",
    "        device (torch.device): 设备（CPU或GPU）。\n",
    "        index (faiss.Index): FAISS索引。\n",
    "        cleaned_documents (list): 清洗后的文档块列表。\n",
    "        top_k (int): 检索的相关文档块数量。\n",
    "\n",
    "    Returns:\n",
    "        str: 生成的回答。\n",
    "    \"\"\"\n",
    "    retrieved_docs = retrieve_documents(query, embedding_model, index, cleaned_documents, top_k, threshold)\n",
    "    answer = generate_answer(query, retrieved_docs, tokenizer, model, device)\n",
    "    return answer\n",
    "\n",
    "def main():\n",
    "    # 设置路径\n",
    "    knowledge_base_dir = '/kaggle/input/myragtxt'  # 替换为您的知识库目录路径\n",
    "    faiss_index_path = '/kaggle/working/faiss_index_ivf.index'\n",
    "    # 替换为实际的本地生成模型路径\n",
    "    local_model_path = '/kaggle/input/lora/pytorch/default/1' \n",
    "    #local_model_path = '/kaggle/input/qwen2.5/transformers/3b-instruct/1' \n",
    "    \n",
    "    \n",
    "    # 1. 加载和预处理文档\n",
    "    print(\"加载文档中...\")\n",
    "    # 设置文档分块参数\n",
    "    chunk_size = 1000  # 每个块的字符数\n",
    "    overlap = 200       # 块之间的重叠字符数\n",
    "    documents = load_documents(knowledge_base_dir, chunk_size=chunk_size, overlap=overlap)\n",
    "    print(f\"已加载并分块 {len(documents)} 个文档块。\")\n",
    "    \n",
    "    print(\"清洗文档中...\")\n",
    "    cleaned_documents = [clean_text(doc) for doc in documents]\n",
    "    \n",
    "    # 2. 加载嵌入模型和生成模型\n",
    "    print(\"加载嵌入模型...\")\n",
    "    # 使用适合中文的 Sentence-Transformer 模型\n",
    "    embedding_model = SentenceTransformer('shibing624/text2vec-base-chinese')  # 你可以根据需要选择其他模型\n",
    "    \n",
    "    print(\"加载生成模型和分词器...\")\n",
    "    # 加载生成模型和分词器\n",
    "    tokenizer = AutoTokenizer.from_pretrained(local_model_path, use_fast=False)\n",
    "    model = AutoModelForCausalLM.from_pretrained(local_model_path)\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # 3. 生成嵌入向量\n",
    "    print(\"生成文档块嵌入向量中...\")\n",
    "    # 使用批量编码提高效率\n",
    "    document_embeddings = embedding_model.encode(cleaned_documents, normalize_embeddings=True, show_progress_bar=True)\n",
    "    document_embeddings = np.array(document_embeddings).astype('float32')\n",
    "    print(f\"已生成 {len(document_embeddings)} 个文档块的嵌入向量。\")\n",
    "    \n",
    "    # 4. 构建 FAISS 索引\n",
    "    print(\"构建 FAISS 索引中...\")\n",
    "    dimension = document_embeddings.shape[1]\n",
    "    \n",
    "    if len(document_embeddings) == 0:\n",
    "        print(\"没有文档块可供索引。程序退出。\")\n",
    "        return\n",
    "    elif len(document_embeddings) < 100:\n",
    "        # 如果文档块数量少于100，使用 IndexFlatIP\n",
    "        print(\"文档块数量少于100。使用 IndexFlatIP 索引。\")\n",
    "        index = faiss.IndexFlatIP(dimension)\n",
    "        index.add(document_embeddings)\n",
    "    else:\n",
    "        # 否则，使用 IndexIVFFlat 并设置合适的 nlist\n",
    "        nlist = min(100, max(100, len(document_embeddings) // 10))  # 一般 nlist = num_clusters\n",
    "        quantizer = faiss.IndexFlatIP(dimension)  # 内积量度\n",
    "        index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "        # 训练索引\n",
    "        print(\"训练 FAISS 索引中...\")\n",
    "        index.train(document_embeddings)\n",
    "        print(\"FAISS 索引已训练。\")\n",
    "        # 添加向量到索引\n",
    "        index.add(document_embeddings)\n",
    "    \n",
    "    # 保存索引\n",
    "    faiss.write_index(index, faiss_index_path)\n",
    "    print(f\"FAISS 索引已保存到 {faiss_index_path}。\")\n",
    "    # 如果需要加载索引，可以使用以下代码\n",
    "    # index = faiss.read_index(faiss_index_path)\n",
    "    \n",
    "    # 5. 测试聊天功能\n",
    "    print(\"欢迎使用 Qwen 聊天机器人！输入内容即可开始对话。\\n输入 \\\\quit 结束会话。\\n\")\n",
    "    while True:\n",
    "        # 用户输入\n",
    "        user_query = input(\"用户: \").strip()\n",
    "        \n",
    "        # 退出会话\n",
    "        if user_query.lower() == r\"\\quit\":\n",
    "            print(\"聊天机器人已退出，会话结束。\")\n",
    "            break\n",
    "        \n",
    "        # 输出用户输入\n",
    "        print(f\"用户: {user_query}\")\n",
    "        \n",
    "        # 获取助手的回应\n",
    "        response = chat(user_query, embedding_model, tokenizer, model, device, index, cleaned_documents, top_k=1)\n",
    "        \n",
    "        # 输出助手回应\n",
    "        print(f\"助手: {response}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
