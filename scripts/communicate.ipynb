{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验0.5b-微调后模型，完成基础的多轮对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\p'\n",
      "C:\\Windows\\Temp\\ipykernel_12388\\210006252.py:7: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  model_path = \"D:\\上海交大本科阶段\\大三上\\自然语言处理\\project\\lora\"\n",
      "d:\\ana\\envs\\machinelearn\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.61it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 确保CUDA可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 指定模型路径\n",
    "model_path = \"D:\\上海交大本科阶段\\大三上\\自然语言处理\\project\\lora\"\n",
    "# 加载微调后模型 (SFT) 和 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    device_map=None, \n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多轮对话    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ana\\envs\\machinelearn\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Thread\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 创建 TextStreamer 实例\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m streamer \u001b[38;5;241m=\u001b[39m TextIteratorStreamer(\u001b[43mtokenizer\u001b[49m, skip_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 临时存储流式生成的文本\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 全局对话历史\u001b[39;00m\n\u001b[0;32m      9\u001b[0m conversation_history \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "# 创建 TextStreamer 实例\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "# 临时存储流式生成的文本\n",
    "# 全局对话历史\n",
    "conversation_history = []\n",
    "\n",
    "# 模型支持的最大 token 长度\n",
    "MAX_MODEL_LENGTH = model.config.max_position_embeddings\n",
    "print(\"MAX History length:\", MAX_MODEL_LENGTH)\n",
    "# 聊天机器人主函数\n",
    "def chatbot():\n",
    "    global conversation_history  # 声明使用全局变量\n",
    "    print(\"欢迎使用 Qwen 聊天机器人！输入内容即可开始对话。\\n输入 \\\\quit 结束会话，输入 \\\\newsession 清空对话历史并重新开始。\\n\")\n",
    "\n",
    "    while True:\n",
    "        # 用户输入\n",
    "        user_input = input(\"用户: \").strip()\n",
    "        \n",
    "        print(\"用户:\", user_input)\n",
    "        # 退出会话\n",
    "        if user_input == r\"\\quit\":\n",
    "            print(\"聊天机器人已退出，会话结束。\")\n",
    "            break\n",
    "        \n",
    "        # 开启新会话\n",
    "        elif user_input == r\"\\newsession\":\n",
    "            conversation_history = []\n",
    "            print(\"已清空对话历史，开启新的对话会话！\")\n",
    "            continue\n",
    "        \n",
    "        # 普通对话\n",
    "        else:\n",
    "            # 添加用户输入到对话历史\n",
    "            conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "            trim_conversation_history()  # 裁剪对话历史\n",
    "            # 构造模型输入（整合你提供的逻辑）\n",
    "            text = tokenizer.apply_chat_template(\n",
    "                format_messages(conversation_history),\n",
    "                tokenize=False,  # 不立即分词\n",
    "                add_generation_prompt=True  # 添加生成提示\n",
    "            )\n",
    "\n",
    "            # 模型推理\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "            \n",
    "            \n",
    "            generation_kwargs = dict(inputs, max_new_tokens=400,  # 限制生成长度\n",
    "                # max_length=1024,  # 限制最大长度\n",
    "                do_sample= True,\n",
    "                temperature=0.2,  # 调整生成的随机性\n",
    "                top_k=5,  # 限制高概率单词的候选范围\n",
    "                top_p=0.95,  # 核采样\n",
    "                repetition_penalty=1.1,  # 惩罚重复生成\n",
    "                eos_token_id=tokenizer.eos_token_id,  # 设置结束标记\n",
    "                pad_token_id=tokenizer.eos_token_id,  # 设置填充标记\n",
    "                streamer=streamer  # 设置 streamer 用于流式输出)\n",
    "            )\n",
    "            \n",
    "            thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "            thread.start()\n",
    "            generated_text = \"\"\n",
    "            print (\"助手: \", end=\"\", flush=True)\n",
    "            for new_text in streamer:\n",
    "                generated_text += new_text\n",
    "                print(new_text, end=\"\", flush=True)\n",
    "            \n",
    "            # 解码生成的响应\n",
    "            # response = tokenizer.decode(outputs[0, inputs[\"input_ids\"].size(1):], skip_special_tokens=True).strip()\n",
    "            # print(\"助手回复:\", response)\n",
    "            \n",
    "            # 添加助手响应到对话历史\n",
    "            conversation_history.append({\"role\": \"assistant\", \"content\": generated_text})\n",
    "            trim_conversation_history()\n",
    "            # print(\"\\n对话历史:\", conversation_history)\n",
    "            # 计算对话历史长度\n",
    "            formatted_history = tokenizer.apply_chat_template(\n",
    "                format_messages(conversation_history),\n",
    "                tokenize=False,  # 返回未分词的字符串\n",
    "                add_generation_prompt=False\n",
    "            )\n",
    "            \n",
    "            # 对格式化后的字符串进行分词，获取 input_ids\n",
    "            tokenized_history = tokenizer(formatted_history, return_tensors=\"pt\", padding=True)\n",
    "            input_length = tokenized_history[\"input_ids\"].shape[1]\n",
    "            print(\"\\n对话历史长度:\", input_length)\n",
    "            \n",
    "            print(\"\\n --------------------\")\n",
    "            # 如果对话历史超出模型最大长度，则裁剪对话历史\n",
    "            \n",
    "\n",
    "\n",
    "# 格式化对话历史为 Qwen 支持的模板\n",
    "def format_messages(history):\n",
    "    \"\"\"\n",
    "    将对话历史格式化为模型支持的模板格式\n",
    "    \"\"\"\n",
    "    formatted_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are helpful and intelligent.\"}\n",
    "    ]  # 初始包含系统指令\n",
    "    formatted_messages.extend(history)  # 添加用户和助手的对话\n",
    "    return formatted_messages\n",
    "\n",
    "# 裁剪对话历史，保证长度不超过模型支持的最大长度\n",
    "def trim_conversation_history():\n",
    "    \"\"\"\n",
    "    当对话历史长度超过模型支持的最大长度时，裁剪最早的对话\n",
    "    \"\"\"\n",
    "    global conversation_history\n",
    "    \n",
    "    # 使用 apply_chat_template 格式化对话历史\n",
    "    formatted_history = tokenizer.apply_chat_template(\n",
    "        format_messages(conversation_history),\n",
    "        tokenize=False,  # 返回未分词的字符串\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    \n",
    "    # 对格式化后的字符串进行分词，获取 input_ids\n",
    "    tokenized_history = tokenizer(formatted_history, return_tensors=\"pt\", padding=True)\n",
    "    input_length = tokenized_history[\"input_ids\"].shape[1]\n",
    "\n",
    "    # 如果长度未超出最大值，不做裁剪\n",
    "    if input_length <= MAX_MODEL_LENGTH:\n",
    "        return\n",
    "\n",
    "    # 超出长度时，逐步移除最早的对话，直到满足长度限制\n",
    "    while input_length > MAX_MODEL_LENGTH:\n",
    "        if conversation_history:  # 确保历史不为空\n",
    "            conversation_history.pop(0)  # 移除最早的对话\n",
    "        # 更新格式化后的对话历史并重新计算长度\n",
    "        formatted_history = tokenizer.apply_chat_template(\n",
    "            format_messages(conversation_history),\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        tokenized_history = tokenizer(formatted_history, return_tensors=\"pt\", padding=True)\n",
    "        input_length = tokenized_history[\"input_ids\"].shape[1]\n",
    "        # print(\"裁剪对话历史，当前长度:\", input_length)\n",
    "# 启动聊天机器人\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
