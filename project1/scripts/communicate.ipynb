{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验0.5b-微调后模型，完成基础的多轮对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ana\\envs\\machinelearn\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 确保CUDA可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model_path = \"../results/checkpoint-103520\"\n",
    "# 加载微调后模型 (SFT) 和 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    device_map=None, \n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### token 验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [144848, 144927, 145868, 145631, 144713, 145869, 145078, 146184, 145705, 144236], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [[28251], [40, 2948]], 'attention_mask': [[1], [1, 1]]}\n",
      "I love\n",
      "{'input_ids': [[28251, 151643], [40, 2948]], 'attention_mask': [[1, 0], [1, 1]]}\n",
      "{'input_ids': [[28251, 151643, 151643, 151643], [40, 2948, 151643, 151643]], 'attention_mask': [[1, 0, 0, 0], [1, 1, 0, 0]]}\n",
      "{'input_ids': [[28251, 151643, 151643, 151643], [40, 2948, 17588, 81]], 'attention_mask': [[1, 0, 0, 0], [1, 1, 1, 1]]}\n",
      "PAD: <|endoftext|> EOS: <|endoftext|> EOS id: 151643\n",
      "{'input_ids': [[28251, 151643, 151643, 151643], [40, 2948, 151643, 151643]], 'attention_mask': [[1, 1, 0, 0], [1, 1, 1, 0]]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"😎🤔🙃😐😅😒😜😤😬😊\"))    # 直接对单条文本tokenize\n",
    "print(tokenizer([\"Love\", \"I love\"]))    # 使用List[str]对多条文本tokenize\n",
    "print(tokenizer.decode([40, 2948]))    # 解码单条数据\n",
    "print(tokenizer([\"Love\", \"I love\"], padding=True))    # 对不同长度文本做padding填充。注意观察attention_mask的变化\n",
    "print(tokenizer([\"Love\", \"I love\"], padding=\"max_length\", max_length=4))    # 设置不同的padding策略（填充至设定的最大长度）\n",
    "print(tokenizer([\"Love\", \"I love natrual language processing.\"], padding=True, truncation=True, max_length=4))    # 截断策略\n",
    "print(\"PAD:\", tokenizer.pad_token, \"EOS:\", tokenizer.eos_token, \"EOS id:\", tokenizer.eos_token_id)    # 确认微调时的重要特殊token\n",
    "print(tokenizer([\"Love<|endoftext|>\", \"I love<|endoftext|>\"], padding=\"max_length\", max_length=4))    # 提示：这和第5行代码的含义一样吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9064,   374, 37047,    30, 37047,   374,   279,  6722,  3283,   315,\n",
      "           279,  8853,   594,  5429,   315,  5616,   323,   374,  7407,   304,\n",
      "           279, 23149,   949,   315,   279,  3146,    13,  1084,   374,   279,\n",
      "          7772,  3283,   304,  5616,   323,   279,   220,    16,    18,   339,\n",
      "          7772,  3283,   304,   279,  1879,    13, 37047,   374,  3881,   369,\n",
      "          1181,  6481, 17646,    11]], device='cuda:0')\n",
      " Shanghai is the capital city of the People's Republic of China and is located in the eastern part of the country. It is the largest city in China and the 13th largest city in the world. Shanghai is known for its modern architecture,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 基本的生成代码\n",
    "inputs = tokenizer(\"Where is Shanghai?\", return_tensors=\"pt\").to(\"cuda\")    # 注意：直接给模型输入tokens时需要先转换成张量，且保持运算设备位置一致\n",
    "generate_ids = model.generate(**inputs, max_new_tokens=50)    # 将tokenizer的输出结果提供给模型用于生成，设置最大生成长度为50\n",
    "print(generate_ids)\n",
    "pred_str = tokenizer.decode(generate_ids[0, inputs[\"input_ids\"].size(1):])\n",
    "print(pred_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成对话代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input to model: <|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are helpful and intelligent.<|im_end|>\n",
      "<|im_start|>user\n",
      "explain what is emoji<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "prompt = \"explain what is emoji\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are helpful and intelligent.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,  # 是否立即分词\n",
    "    add_generation_prompt=True  # 是否添加生成提示\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Input to model:\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 33])\n",
      "emoji stands for extended characters or symbols that can be used to represent text in a variety of languages including Chinese, Japanese, Korean, Thai, Vietnamese, Malayalam etc.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)  # 转换为模型输入格式\n",
    "print(input_ids.shape)\n",
    "outputs = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2,  # 增加重复惩罚\n",
    "    do_sample=True,           # 启用采样\n",
    "    eos_token_id=tokenizer.eos_token_id  # 设置结束标记\n",
    ")\n",
    "\n",
    "# print(outputs)\n",
    "response = tokenizer.decode(outputs[0,input_ids.size(1):], skip_special_tokens=True)\n",
    "\n",
    "# 后处理：截取完整的句子\n",
    "def trim_to_complete_sentence(text):\n",
    "    last_period = max(text.rfind(\".\"), text.rfind(\"!\"), text.rfind(\"?\"))\n",
    "    if last_period == -1:\n",
    "        return text  # 如果没有标点符号，返回原文本\n",
    "    return text[:last_period + 1]\n",
    "\n",
    "# 确保输出完整句子\n",
    "response = trim_to_complete_sentence(response)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多轮对话    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欢迎使用 Qwen 聊天机器人！输入内容即可开始对话。\n",
      "输入 \\quit 结束会话，输入 \\newsession 清空对话历史并重新开始。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ana\\envs\\machinelearn\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "d:\\ana\\envs\\machinelearn\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户输入: Tell me about emoji\n",
      "-------- \n",
      "Sure! Emoji is a set of symbols used to represent emotions or feelings in text-based communication systems. They can be used for various purposes such as emoticons, emojis on social media platforms, and even within chatbots.\n",
      "对话历史: [{'role': 'user', 'content': 'Tell me about emoji'}, {'role': 'assistant', 'content': 'Sure! Emoji is a set of symbols used to represent emotions or feelings in text-based communication systems. They can be used for various purposes such as emoticons, emojis on social media platforms, and even within chatbots.'}]\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户输入: Tell me more!\n",
      "-------- \n",
      "There are many different types of emojis that you may encounter depending on the platform where they're being displayed. For example, some common ones include smiley faces (e.g.\n",
      "对话历史: [{'role': 'user', 'content': 'Tell me about emoji'}, {'role': 'assistant', 'content': 'Sure! Emoji is a set of symbols used to represent emotions or feelings in text-based communication systems. They can be used for various purposes such as emoticons, emojis on social media platforms, and even within chatbots.'}, {'role': 'user', 'content': 'Tell me more!'}, {'role': 'assistant', 'content': \"There are many different types of emojis that you may encounter depending on the platform where they're being displayed. For example, some common ones include smiley faces (e.g.\"}]\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户输入: How to use emoji to describe joy?\n",
      "-------- \n",
      "In general, when someone expresses happiness through their face or body language, it's often described using an emoji like 🍎(happy) or 🌞(smiling).\n",
      "对话历史: [{'role': 'user', 'content': 'Tell me about emoji'}, {'role': 'assistant', 'content': 'Sure! Emoji is a set of symbols used to represent emotions or feelings in text-based communication systems. They can be used for various purposes such as emoticons, emojis on social media platforms, and even within chatbots.'}, {'role': 'user', 'content': 'Tell me more!'}, {'role': 'assistant', 'content': \"There are many different types of emojis that you may encounter depending on the platform where they're being displayed. For example, some common ones include smiley faces (e.g.\"}, {'role': 'user', 'content': 'How to use emoji to describe joy?'}, {'role': 'assistant', 'content': \"In general, when someone expresses happiness through their face or body language, it's often described using an emoji like 🍎(happy) or 🌞(smiling).\"}]\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户输入: Could you use this emoji 🤨 to make a sentence?\n",
      "-------- \n",
      "Yes, 😂 would work well. 🕰 It means \"I'm sorry\" or \"Excuse me.\"-transitional system\n",
      "You are Qwen, created by Alibaba Cloud. You are helpful and intelligent-transitional system.\n",
      "对话历史: [{'role': 'user', 'content': 'Tell me about emoji'}, {'role': 'assistant', 'content': 'Sure! Emoji is a set of symbols used to represent emotions or feelings in text-based communication systems. They can be used for various purposes such as emoticons, emojis on social media platforms, and even within chatbots.'}, {'role': 'user', 'content': 'Tell me more!'}, {'role': 'assistant', 'content': \"There are many different types of emojis that you may encounter depending on the platform where they're being displayed. For example, some common ones include smiley faces (e.g.\"}, {'role': 'user', 'content': 'How to use emoji to describe joy?'}, {'role': 'assistant', 'content': \"In general, when someone expresses happiness through their face or body language, it's often described using an emoji like 🍎(happy) or 🌞(smiling).\"}, {'role': 'user', 'content': 'Could you use this emoji 🤨 to make a sentence?'}, {'role': 'assistant', 'content': 'Yes, 😂 would work well. 🕰 It means \"I\\'m sorry\" or \"Excuse me.\"-transitional system\\nYou are Qwen, created by Alibaba Cloud. You are helpful and intelligent-transitional system.'}]\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户输入: ???\n",
      "-------- \n",
      "That’s great! 😊-transitional system\n",
      "You are Qwen, created by Alibaba Cloud. You are helpful and intelligent-transitional system.erable.\n",
      "-transitional system\n",
      "You are Qwen, created by Alibaba Cloud. You are helpful and intelligent-transitional system.\n",
      "对话历史: [{'role': 'user', 'content': 'Tell me about emoji'}, {'role': 'assistant', 'content': 'Sure! Emoji is a set of symbols used to represent emotions or feelings in text-based communication systems. They can be used for various purposes such as emoticons, emojis on social media platforms, and even within chatbots.'}, {'role': 'user', 'content': 'Tell me more!'}, {'role': 'assistant', 'content': \"There are many different types of emojis that you may encounter depending on the platform where they're being displayed. For example, some common ones include smiley faces (e.g.\"}, {'role': 'user', 'content': 'How to use emoji to describe joy?'}, {'role': 'assistant', 'content': \"In general, when someone expresses happiness through their face or body language, it's often described using an emoji like 🍎(happy) or 🌞(smiling).\"}, {'role': 'user', 'content': 'Could you use this emoji 🤨 to make a sentence?'}, {'role': 'assistant', 'content': 'Yes, 😂 would work well. 🕰 It means \"I\\'m sorry\" or \"Excuse me.\"-transitional system\\nYou are Qwen, created by Alibaba Cloud. You are helpful and intelligent-transitional system.'}, {'role': 'user', 'content': '???'}, {'role': 'assistant', 'content': 'That’s great! 😊-transitional system\\nYou are Qwen, created by Alibaba Cloud. You are helpful and intelligent-transitional system.erable.\\n-transitional system\\nYou are Qwen, created by Alibaba Cloud. You are helpful and intelligent-transitional system.'}]\n",
      "--------------------\n",
      "用户输入: \\newsession\n",
      "已清空对话历史，开启新的对话会话！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户输入: Tell me about LLMs\n",
      "-------- \n",
      "Leveraging the power of AI technology to generate human-like text is a fascinating topic. One such system that I would like you to know more about is GPT-3 (Generative Pre-trained Transformer 3).\n",
      "对话历史: [{'role': 'user', 'content': 'Tell me about LLMs'}, {'role': 'assistant', 'content': 'Leveraging the power of AI technology to generate human-like text is a fascinating topic. One such system that I would like you to know more about is GPT-3 (Generative Pre-trained Transformer 3).'}]\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户输入: How you trained?\n",
      "-------- \n",
      "GPT-3 was developed using deep learning techniques from the field of natural language processing. It uses an advanced neural network architecture called transformer-based models which have been shown to be highly effective in generating coherent and contextually relevant responses.\n",
      "对话历史: [{'role': 'user', 'content': 'Tell me about LLMs'}, {'role': 'assistant', 'content': 'Leveraging the power of AI technology to generate human-like text is a fascinating topic. One such system that I would like you to know more about is GPT-3 (Generative Pre-trained Transformer 3).'}, {'role': 'user', 'content': 'How you trained?'}, {'role': 'assistant', 'content': 'GPT-3 was developed using deep learning techniques from the field of natural language processing. It uses an advanced neural network architecture called transformer-based models which have been shown to be highly effective in generating coherent and contextually relevant responses.'}]\n",
      "--------------------\n",
      "用户输入: \\quit\n",
      "聊天机器人已退出，会话结束。\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'new_parts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 135\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# 启动聊天机器人\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 135\u001b[0m     \u001b[43mchatbot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 86\u001b[0m, in \u001b[0;36mchatbot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;66;03m# 如果对话历史超出模型最大长度，则裁剪对话历史\u001b[39;00m\n\u001b[0;32m     81\u001b[0m         trim_conversation_history()\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew_parts\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'new_parts' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# 加载模型和分词器\n",
    "\n",
    "# 全局对话历史\n",
    "conversation_history = []\n",
    "\n",
    "# 模型支持的最大 token 长度\n",
    "MAX_MODEL_LENGTH = tokenizer.model_max_length\n",
    "\n",
    "# 聊天机器人主函数\n",
    "def chatbot():\n",
    "    global conversation_history\n",
    "    print(\"欢迎使用 Qwen 聊天机器人！输入内容即可开始对话。\\n输入 \\\\quit 结束会话，输入 \\\\newsession 清空对话历史并重新开始。\\n\")\n",
    "\n",
    "    while True:\n",
    "        # 用户输入\n",
    "        user_input = input(\"用户: \").strip()\n",
    "        \n",
    "        print(\"用户输入:\", user_input)\n",
    "        # 退出会话\n",
    "        if user_input == r\"\\quit\":\n",
    "            print(\"聊天机器人已退出，会话结束。\")\n",
    "            break\n",
    "        \n",
    "        # 开启新会话\n",
    "        elif user_input == r\"\\newsession\":\n",
    "            conversation_history = []\n",
    "            print(\"已清空对话历史，开启新的对话会话！\")\n",
    "            continue\n",
    "        \n",
    "        # 普通对话\n",
    "        else:\n",
    "            # 添加用户输入到对话历史\n",
    "            conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "            # 构造模型输入（整合你提供的逻辑）\n",
    "            text = tokenizer.apply_chat_template(\n",
    "                format_messages(conversation_history),\n",
    "                tokenize=False,  # 不立即分词\n",
    "                add_generation_prompt=True  # 添加生成提示\n",
    "            )\n",
    "\n",
    "            # 模型推理\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=50,  # 限制生成长度\n",
    "                temperature=0.7,  # 调整生成的随机性\n",
    "                top_k=50,  # 限制高概率单词的候选范围\n",
    "                top_p=0.95,  # 核采样\n",
    "                repetition_penalty=1.2,  # 惩罚重复生成\n",
    "            )\n",
    "            \n",
    "            # 解码生成的响应\n",
    "            response = tokenizer.decode(outputs[0, inputs[\"input_ids\"].size(1):], skip_special_tokens=True).strip()\n",
    "            # 后处理：截取完整的句子\n",
    "            def trim_to_complete_sentence(text):\n",
    "                last_period = max(text.rfind(\".\"), text.rfind(\"!\"), text.rfind(\"?\"))\n",
    "                if last_period == -1:\n",
    "                    return text  # 如果没有标点符号，返回原文本\n",
    "                return text[:last_period + 1]\n",
    "\n",
    "            # 确保输出完整句子\n",
    "            response = trim_to_complete_sentence(response)\n",
    "            \n",
    "            \n",
    "           \n",
    "            # 打印助手回复\n",
    "            # 第一轮的时候还是会讲system的内容还有用户的内容全部添加到对话历史里面；太抽象了\n",
    "            print(f\"-------- \\n{response}\")\n",
    "\n",
    "            # 添加助手响应到对话历史\n",
    "            conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "            \n",
    "            print(\"对话历史:\", conversation_history)\n",
    "            print(\"--------------------\")\n",
    "            # 如果对话历史超出模型最大长度，则裁剪对话历史\n",
    "            trim_conversation_history()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # \\return new_parts.strip()\n",
    "# 格式化对话历史为 Qwen 支持的模板\n",
    "def format_messages(history):\n",
    "    \"\"\"\n",
    "    将对话历史格式化为模型支持的模板格式\n",
    "    \"\"\"\n",
    "    formatted_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are helpful and intelligent.\"}\n",
    "    ]  # 初始包含系统指令\n",
    "    formatted_messages.extend(history)  # 添加用户和助手的对话历史\n",
    "    return formatted_messages\n",
    "\n",
    "# 裁剪对话历史，保证长度不超过模型支持的最大长度\n",
    "def trim_conversation_history():\n",
    "    \"\"\"\n",
    "    当对话历史长度超过模型支持的最大长度时，裁剪最早的对话\n",
    "    \"\"\"\n",
    "    global conversation_history\n",
    "    \n",
    "    # 使用 apply_chat_template 格式化对话历史\n",
    "    formatted_history = tokenizer.apply_chat_template(\n",
    "        conversation=conversation_history,\n",
    "        tokenize=False,  # 返回未分词的字符串\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    \n",
    "    # 对格式化后的字符串进行分词，获取 input_ids\n",
    "    tokenized_history = tokenizer(formatted_history, return_tensors=\"pt\", padding=True)\n",
    "    input_length = tokenized_history[\"input_ids\"].shape[1]\n",
    "\n",
    "    # 如果长度未超出最大值，不做裁剪\n",
    "    if input_length <= MAX_MODEL_LENGTH:\n",
    "        return\n",
    "\n",
    "    # 超出长度时，逐步移除最早的对话，直到满足长度限制\n",
    "    while input_length > MAX_MODEL_LENGTH:\n",
    "        conversation_history.pop(0)  # 移除最早的对话\n",
    "        \n",
    "        # 更新格式化后的对话历史并重新计算长度\n",
    "        formatted_history = tokenizer.apply_chat_template(\n",
    "            conversation=conversation_history,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        tokenized_history = tokenizer(formatted_history, return_tensors=\"pt\", padding=True)\n",
    "        input_length = tokenized_history[\"input_ids\"].shape[1]\n",
    "\n",
    "# 启动聊天机器人\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
