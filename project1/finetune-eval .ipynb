{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装opencompass：Kaggle上已经为我们准备好了其他常用包，只需安装opencompass用于评测即可。如果不在Kaggle上运行，则还需要安装其他必要包。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 指令微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T13:59:28.707719Z",
     "iopub.status.busy": "2024-12-25T13:59:28.707158Z",
     "iopub.status.idle": "2024-12-25T13:59:28.713261Z",
     "shell.execute_reply": "2024-12-25T13:59:28.712345Z",
     "shell.execute_reply.started": "2024-12-25T13:59:28.707679Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ana\\envs\\machinelearn\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The main program for finetuning LLMs with Huggingface Transformers Library.\n",
    "\n",
    "ALL SECTIONS WHERE CODE POSSIBLY NEEDS TO BE FILLED IN ARE MARKED AS TODO.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Dict\n",
    "import sys\n",
    "import torch\n",
    "from transformers import TrainingArguments, HfArgumentParser, Trainer, AutoTokenizer, AutoModelForCausalLM\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T13:59:45.053375Z",
     "iopub.status.busy": "2024-12-25T13:59:45.052435Z",
     "iopub.status.idle": "2024-12-25T13:59:45.061188Z",
     "shell.execute_reply": "2024-12-25T13:59:45.060236Z",
     "shell.execute_reply.started": "2024-12-25T13:59:45.053329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"Arguments for model.\"\"\"\n",
    "    model_name_or_path: str = field(\n",
    "        default=\"/kaggle/input/Qwen-2.5-0.5B\",\n",
    "        metadata={\"help\": \"The path to the LLM to fine-tune or its name on the Hugging Face Hub.\"}\n",
    "    )\n",
    "    torch_dtype: str = field(\n",
    "        default=\"bfloat16\",\n",
    "        metadata={\n",
    "            \"help\": \"The precision to use when loading the model.\",\n",
    "            \"choices\": [\"bfloat16\", \"float16\", \"float32\"]\n",
    "        },\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    \"\"\"Arguments for data.\"\"\"\n",
    "    dataset_path: str = field(\n",
    "        default=\"alpaca-cleaned\",\n",
    "        metadata={\"help\": \"The path to the fine-tuning dataset or its name on the Hugging Face Hub.\"}\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=1024,\n",
    "        metadata={\"help\": \"The maximum sequence length for the input data.\"}\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T13:59:50.911016Z",
     "iopub.status.busy": "2024-12-25T13:59:50.910561Z",
     "iopub.status.idle": "2024-12-25T13:59:50.922383Z",
     "shell.execute_reply": "2024-12-25T13:59:50.921592Z",
     "shell.execute_reply.started": "2024-12-25T13:59:50.910983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# The main function\n",
    "# NOTE You can customize some logs to monitor your program.\n",
    "def finetune():\n",
    "    # TODO Step 1: Define an arguments parser and parse the arguments\n",
    "    # NOTE Three parts: model arguments, data arguments, and training arguments\n",
    "    # HINT: Refer to \n",
    "    #   * https://huggingface.co/docs/transformers/v4.46.3/en/internal/trainer_utils#transformers.HfArgumentParser\n",
    "    #   * https://huggingface.co/docs/transformers/v4.46.3/en/main_classes/trainer#transformers.TrainingArguments\n",
    "    parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "\n",
    "    # TODO Step 2: Load tokenizer and model\n",
    "    # HINT 1: Refer to\n",
    "    #   * https://huggingface.co/docs/transformers/v4.46.3/en/main_classes/tokenizer#tokenizer\n",
    "    #   * https://huggingface.co/docs/transformers/v4.46.3/en/model_doc/qwen2\n",
    "    # HINT 2: To save training GPU memory, you need to set the model's parameter precision to half-precision (float16 or bfloat16).\n",
    "    #         You may also check other strategies to save the memory!\n",
    "    #   * https://huggingface.co/docs/transformers/v4.46.3/en/model_doc/llama2#usage-tips\n",
    "    #   * https://huggingface.co/docs/transformers/perf_train_gpu_one\n",
    "    #   * https://www.53ai.com/news/qianyanjishu/2024052494875.html\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        torch_dtype=model_args.torch_dtype,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    model.gradient_checkpointing_enable()\n",
    "    print(model.device)\n",
    "\n",
    "    # TODO Step 3: Load dataset\n",
    "    # HINT: https://huggingface.co/docs/datasets/v3.1.0/en/package_reference/main_classes#datasets.Dataset\n",
    "    dataset = datasets.load_dataset(data_args.dataset_path)\n",
    "\n",
    "    # TODO Step 4: Define the data collator function\n",
    "    # NOTE During training, for each model parameter update, we fetch a batch of data, perform a forward and backward pass,\n",
    "    # and then update the model parameters. The role of the data collator is to process the data (e.g., padding the data within\n",
    "    # a batch to the same length) and format the batch into the input required by the model.\n",
    "    #\n",
    "    # In this assignment, the purpose of the custom data_collator is to process each batch of data from the dataset loaded in\n",
    "    # Step 3 into the format required by the model. This includes tasks such as tokenizing the data, converting each token into \n",
    "    # an ID sequence, applying padding, and preparing labels.\n",
    "    # \n",
    "    # HINT:\n",
    "    #   * Before implementation, you should:\n",
    "    #      1. Clearly understand the format of each sample in the dataset loaded in Step 3.\n",
    "    #      2. Understand the input format required by the model (https://huggingface.co/docs/transformers/model_doc/qwen2#transformers.Qwen2ForCausalLM).\n",
    "    #         Reading its source code also helps!\n",
    "\n",
    "    \n",
    "    def data_collator(batch: List[Dict[str, str]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        batch: list of dict, each dict of the list is a sample in the dataset.\n",
    "        \"\"\"\n",
    "        instructions = [sample['instruction'] for sample in batch]\n",
    "        inputs = [sample['input'] for sample in batch]\n",
    "        outputs = [sample['output'] for sample in batch]\n",
    "        input_texts = [f\"{instruction} {input} {output}\" for instruction, input, output in zip(instructions, inputs, outputs)]\n",
    "        tokenized_inputs = tokenizer(\n",
    "        input_texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=tokenizer.model_max_length\n",
    "        )\n",
    "        labels = tokenized_inputs[\"input_ids\"].clone()\n",
    "        for i, feature in enumerate(batch):\n",
    "            instruction_length = len(tokenizer(feature['instruction'], add_special_tokens=False)[\"input_ids\"])\n",
    "             # 检查 input 是否为空字符串\n",
    "            if feature['input'] == None:\n",
    "                feature['input'] = \"\"\n",
    "            input_length = len(tokenizer(feature['input'], add_special_tokens=False)[\"input_ids\"])\n",
    "            output_start = instruction_length + input_length - 1\n",
    "\n",
    "            # Mask everything except the output part with -100\n",
    "            labels[i, :output_start] = -100\n",
    "            # 将padding部分的token也设置为-100\n",
    "            labels[labels == tokenizer.pad_token_id] = -100\n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "        \n",
    "        return tokenized_inputs\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # TODO Step 5: Define the Trainer\n",
    "    # HINT: https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        tokenizer=tokenizer,\n",
    "        \n",
    "    )\n",
    "\n",
    "    # Step 6: Train!\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T13:59:54.946294Z",
     "iopub.status.busy": "2024-12-25T13:59:54.945516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Pass your training arguments.\n",
    "# NOTE [IMPORTANT!!!] DO NOT FORGET TO PASS PROPER ARGUMENTS TO SAVE YOUR CHECKPOINTS!!!\n",
    "sys.argv = [\n",
    "    \"notebook\", \n",
    "    \"--model_name_or_path\", \"qwen2.5\",\n",
    "    \"--dataset_path\", \"Alpaca Cleaned\",\n",
    "    \"--output_dir\", \"working/lr_1e-6_bs_1\",\n",
    "    \"--learning_rate\", \"1e-6\",\n",
    "    \"--num_train_epochs\", \"2\",\n",
    "    \"--per_device_train_batch_size\", \"1\",  # 每个设备的训练批次大小\n",
    "    \"--save_steps\", \"500\",\n",
    "    \"--save_total_limit\", \"1\",\n",
    "    \"--remove_unused_columns\", \"False\",\n",
    "]\n",
    "\n",
    "finetune()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-25T02:42:36.084868Z",
     "iopub.status.idle": "2024-12-25T02:42:36.085173Z",
     "shell.execute_reply": "2024-12-25T02:42:36.085036Z",
     "shell.execute_reply.started": "2024-12-25T02:42:36.085022Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "PLM_MODEL_PATH = \"qwen2.5\"\n",
    "SFT_MODEL_PATH = \"working/lr_1e-6_bs_1/checkpoint-103520\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你有多个GPU，可以修改下面的--hf-num-gpus参数来加速评测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-25T02:42:36.087164Z",
     "iopub.status.idle": "2024-12-25T02:42:36.087991Z",
     "shell.execute_reply": "2024-12-25T02:42:36.087710Z",
     "shell.execute_reply.started": "2024-12-25T02:42:36.087682Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!opencompass \\\n",
    "    --datasets mmlu_ppl hellaswag_clean_ppl winogrande_ll ARC_e_ppl ARC_c_clean_ppl SuperGLUE_BoolQ_few_shot_ppl \\\n",
    "    --summarizer example \\\n",
    "    --hf-type base \\\n",
    "    --hf-path {PLM_MODEL_PATH} \\\n",
    "    --tokenizer-kwargs padding_side=\"left\" truncation=\"left\" \\\n",
    "    --max-seq-len 2048 \\\n",
    "    --batch-size 4 \\\n",
    "    --hf-num-gpus 1 \\\n",
    "    --work-dir \"working/evals/plm\" \\\n",
    "    --debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-25T02:42:36.089291Z",
     "iopub.status.idle": "2024-12-25T02:42:36.089829Z",
     "shell.execute_reply": "2024-12-25T02:42:36.089590Z",
     "shell.execute_reply.started": "2024-12-25T02:42:36.089565Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!opencompass \\\n",
    "    --datasets mmlu_ppl hellaswag_clean_ppl winogrande_ll ARC_e_ppl ARC_c_clean_ppl SuperGLUE_BoolQ_few_shot_ppl \\\n",
    "    --summarizer example \\\n",
    "    --hf-type base \\\n",
    "    --hf-path {SFT_MODEL_PATH} \\\n",
    "    --tokenizer-kwargs padding_side=\"left\" truncation=\"left\" \\\n",
    "    --max-seq-len 2048 \\\n",
    "    --batch-size 4 \\\n",
    "    --hf-num-gpus 2 \\\n",
    "    --work-dir \"working/evals/sft\" \\\n",
    "    --debug"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4061777,
     "sourceId": 7056498,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 141432,
     "sourceId": 166218,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "machinelearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
